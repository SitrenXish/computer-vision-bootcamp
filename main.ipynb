{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author: Ishman Mann\n",
    "# @date: 13/10/2022\n",
    "# \n",
    "# @description:\n",
    "#   Classification model for CIFAR-10 dataset using a CNN in pyTorch\n",
    "#\n",
    "# @resources:\n",
    "#   https://www.learnpytorch.io/\n",
    "#\n",
    "# @notes:\n",
    "#\n",
    "#\n",
    "# @todo:\n",
    "#   Build model\n",
    "#   Train model -- think of where to put softmax, crossEntropyLoss has softmax in it, so I can't \n",
    "#                  put softmax in my model layers. When computing accuracy, just call softmax there \n",
    "#   Create a confusion matrix\n",
    "#   Add image augmentation\n",
    "#   Further hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Magic lines\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Imports\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "from tqdm.auto import tqdm # for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Documents\\\\computer-vision-bootcamp-pyTorch'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device as gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to save models/metadata\n",
    "\n",
    "MODEL_VERSION = '13'\n",
    "modelTopDir = os.path.join(\"./saved_models\", f\"model_v{MODEL_VERSION}\")\n",
    "\n",
    "if (not os.path.exists(modelTopDir)):\n",
    "    modelCheckpointsDir = os.path.join(modelTopDir,'checkpoints') # model checkpoints will be stored here\n",
    "    modelDir = os.path.join(modelTopDir,'model') # final model will be stored here\n",
    "    modelStatsDir = os.path.join(modelTopDir,'stats') # logs dir, pyplot, and train history will be stored here\n",
    "    modelLogsDir = os.path.join(modelStatsDir, 'logs') \n",
    "\n",
    "    os.makedirs(modelCheckpointsDir)\n",
    "    os.makedirs(modelDir)\n",
    "    os.makedirs(modelStatsDir)\n",
    "    os.makedirs(modelLogsDir)\n",
    "else:\n",
    "  raise Exception('Saved model folders for model version manually specified already exist.')\n",
    "\n",
    "# Create trian_history.txt file \n",
    "trainHistoryFilepath = os.path.join(modelStatsDir, \"train_history.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete and recreate datasets folder (For Google Colab only)\n",
    "if os.path.exists(\"./datasets\"):\n",
    "  shutil.rmtree(\"./datasets\", ignore_errors=True)\n",
    "  os.makedirs(\"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Loading testing and training data\n",
    "\n",
    "trainData = datasets.CIFAR10(\n",
    "    root=\"./datasets\",\n",
    "    train=True, # get train data\n",
    "    download=True,\n",
    "    transform=ToTensor(), # converts PIL to torch.tensor\n",
    "    target_transform=None #dont transform targets (labels)!\n",
    ")\n",
    "\n",
    "testData = datasets.CIFAR10(\n",
    "    root=\"./datasets\",\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None #dont transform targets (labels)!\n",
    ") \n",
    "\n",
    "CLASS_NAMES = trainData.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off some validation data\n",
    "TRAIN_LENGTH = int(len(trainData.data)*0.8)\n",
    "VALIDATE_LENGTH = int(len(trainData.data)*0.2)\n",
    "trainData, validateData = torch.utils.data.random_split(trainData, [TRAIN_LENGTH, VALIDATE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing a sample image\n",
    "image, label = trainData[0]\n",
    "imagePermuted = image.permute(1,2,0)\n",
    "print(imagePermuted.shape)\n",
    "plt.imshow(imagePermuted)\n",
    "plt.title(CLASS_NAMES[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the data using DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "trainDataloader = DataLoader(trainData, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validateDataloader = DataLoader(validateData, batch_size=BATCH_SIZE, shuffle=False)\n",
    "testDataloader = DataLoader(testData, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Create the model class\n",
    "\n",
    "class CIFAR10ModelV1(nn.Module):\n",
    "\n",
    "  def __init__(self, inputChannels: int, hiddenUnitsCnn: int, hiddenUnitsFc: int, outputShape: int):\n",
    "    super().__init__()\n",
    "\n",
    "    # Convolution & pooling layers\n",
    "    self.cnn_layer_1 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=inputChannels, out_channels=hiddenUnitsCnn,\n",
    "                kernel_size=3, stride=1, padding=\"same\"),\n",
    "      nn.ReLU(),\n",
    "      nn.BatchNorm2d(num_features=hiddenUnitsCnn, eps=1e-05, momentum=0.1, affine=True),\n",
    "      nn.Conv2d(in_channels=hiddenUnitsCnn, out_channels=hiddenUnitsCnn,\n",
    "                kernel_size=3, stride=1, padding=\"same\"),\n",
    "      nn.ReLU(),\n",
    "      nn.BatchNorm2d(num_features=hiddenUnitsCnn, eps=1e-05, momentum=0.1, affine=True),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      nn.Dropout(p=0.2)\n",
    "    )\n",
    "\n",
    "    self.cnn_layer_2 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=hiddenUnitsCnn, out_channels=2*hiddenUnitsCnn,\n",
    "                kernel_size=3, stride=1, padding=\"same\"),\n",
    "      nn.ReLU(),\n",
    "      nn.BatchNorm2d(num_features=2*hiddenUnitsCnn, eps=1e-05, momentum=0.1, affine=True),\n",
    "      nn.Conv2d(in_channels=2*hiddenUnitsCnn, out_channels=2*hiddenUnitsCnn,\n",
    "                kernel_size=3, stride=1, padding=\"same\"),\n",
    "      nn.ReLU(),\n",
    "      nn.BatchNorm2d(num_features=2*hiddenUnitsCnn, eps=1e-05, momentum=0.1, affine=True),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      nn.Dropout(p=0.2)\n",
    "    )\n",
    "\n",
    "    self.cnn_layer_3 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=2*hiddenUnitsCnn, out_channels=4*hiddenUnitsCnn,\n",
    "                kernel_size=3, stride=1, padding=\"same\"),\n",
    "      nn.ReLU(),\n",
    "      nn.BatchNorm2d(num_features=4*hiddenUnitsCnn, eps=1e-05, momentum=0.1, affine=True),\n",
    "      nn.Conv2d(in_channels=4*hiddenUnitsCnn, out_channels=4*hiddenUnitsCnn,\n",
    "                kernel_size=3, stride=1, padding=\"same\"),\n",
    "      nn.ReLU(),\n",
    "      nn.BatchNorm2d(num_features=4*hiddenUnitsCnn, eps=1e-05, momentum=0.1, affine=True),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      nn.Dropout(p=0.1)\n",
    "    )\n",
    "\n",
    "    # Fully connected (FC) layers\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=4*hiddenUnitsCnn*4*4, # 8*8 comes from maxpooling 32*32 pixels thrice\n",
    "                   out_features=hiddenUnitsFc),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(num_features=hiddenUnitsFc, eps=1e-05, momentum=0.1, affine=True),\n",
    "        nn.Linear(in_features=hiddenUnitsFc,\n",
    "                  out_features=outputShape)\n",
    "    )\n",
    "\n",
    "  # Placing model layers in forward()\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    x = self.cnn_layer_1(x)\n",
    "    x = self.cnn_layer_2(x)\n",
    "    x = self.cnn_layer_3(x)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Instantiate a model\n",
    "modelInst = CIFAR10ModelV1(inputChannels=3, hiddenUnitsCnn=32, hiddenUnitsFc=512, outputShape=len(CLASS_NAMES)).to(DEVICE)\n",
    "modelInst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss function, optimizer, and accuracy function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=modelInst.parameters(), lr=0.0005, weight_decay=0.0025)\n",
    "\n",
    "def accuracy_function(yActual, yPredicted):\n",
    "  return torch.mean(torch.eq(yActual, yPredicted).float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Create the training and testing loop\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "trainStartTime = timer()\n",
    "\n",
    "trainLosses = []\n",
    "trainAccuracies = []\n",
    "testLosses = []\n",
    "testAccuracies = []\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "  epochStartTime = timer()\n",
    "  epochHeaderMsg = f\"Epoch: {epoch}\\n------------------\"\n",
    "  print(epochHeaderMsg)\n",
    " \n",
    "  ################################################################################\n",
    "  # Training loop\n",
    "  trainLoss, trainAccuracy = 0, 0\n",
    "  modelInst.train()\n",
    "  for xTrain, yTrain in trainDataloader:\n",
    "    xTrain, yTrain = xTrain.to(DEVICE), yTrain.to(DEVICE)\n",
    "    currentBatchSize = len(xTrain)\n",
    "    # 1 - Forward pass\n",
    "    trainLogits = modelInst(xTrain)\n",
    "    trainPredictions = torch.softmax(trainLogits, dim=1).argmax(dim=1) # gets 1D tensor of predicted classes\n",
    "    # 2 - Compute batch loss, accuracy and accumulate into trainLoss, trainAccuracy\n",
    "    loss = loss_function(trainLogits, yTrain) # use logits \n",
    "    trainLoss += loss*currentBatchSize\n",
    "    trainAccuracy += accuracy_function(yActual=yTrain, yPredicted=trainPredictions)*currentBatchSize\n",
    "    # 3 - Set gradients to 0\n",
    "    optimizer.zero_grad()\n",
    "    # 4 - Compute loss gradients\n",
    "    loss.backward()\n",
    "    # 5 - Optimizer (This one uses gradient descent)\n",
    "    optimizer.step()\n",
    "  \n",
    "  # Divide accumulated accuracy/loss by number of individual images, and print\n",
    "  trainLoss /= TRAIN_LENGTH\n",
    "  trainAccuracy /= TRAIN_LENGTH\n",
    "  trainLosses.append(trainLoss)\n",
    "  trainAccuracies.append(trainAccuracy)\n",
    "  trainMsg = f\"Train loss: {trainLoss:.5f}, Train accuracy: {trainAccuracy:.5f}%,\\n\"\n",
    "  print(trainMsg)\n",
    "\n",
    "  ################################################################################\n",
    "  # Testing loop\n",
    "  testLoss, testAccuracy = 0, 0\n",
    "  modelInst.eval()\n",
    "  with torch.inference_mode():\n",
    "    for xValidate, yValidate in validateDataloader:\n",
    "      xValidate, yValidate = xValidate.to(DEVICE), yValidate.to(DEVICE)\n",
    "      currentBatchSize = len(xValidate)\n",
    "      # 1 - Forward pass\n",
    "      testLogits = modelInst(xValidate)\n",
    "      testPredictions = torch.softmax(testLogits, dim=1).argmax(dim=1) \n",
    "      # 2 - Compute batch loss, accuracy and accumulate into testLoss, testAccuracy\n",
    "      testLoss += loss_function(testLogits, yValidate)*currentBatchSize # use logits\n",
    "      testAccuracy += accuracy_function(yActual=yValidate, yPredicted=testPredictions)*currentBatchSize\n",
    "    \n",
    "    # Divide accumulated accuracy/loss by number of individual images, and print\n",
    "    testLoss /= VALIDATE_LENGTH\n",
    "    testAccuracy /= VALIDATE_LENGTH\n",
    "    testLosses.append(testLoss)\n",
    "    testAccuracies.append(testAccuracy)\n",
    "    testMsg = f\"Test loss: {testLoss:.5f}, Test accuracy: {testAccuracy:.5f}%\\n\"\n",
    "    print(testMsg)\n",
    "\n",
    "  ################################################################################\n",
    "  # Each epoch, print epoch train time\n",
    "  epochEndTime = timer()\n",
    "  epochTimeMsg = f\"Train time: {(epochEndTime-epochStartTime):.5f} sec\\n\"\n",
    "  print(epochTimeMsg)\n",
    "\n",
    "  # Save accuracy and loss in logs\n",
    "  try:\n",
    "    writer = SummaryWriter(modelLogsDir)\n",
    "    writer.add_scalar('Loss/train', trainLoss, epoch)\n",
    "    writer.add_scalar('Loss/test', testLoss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', trainAccuracy, epoch)\n",
    "    writer.add_scalar('Accuracy/test', testAccuracy, epoch)\n",
    "    writer.close()\n",
    "  except:\n",
    "    pass    \n",
    "\n",
    "  # Save history info\n",
    "  try: \n",
    "    trainHistoryFile = open(trainHistoryFilepath, \"a\")\n",
    "    trainHistoryFile.write(epochHeaderMsg)\n",
    "    trainHistoryFile.write(trainMsg)\n",
    "    trainHistoryFile.write(testMsg)\n",
    "    trainHistoryFile.write(epochTimeMsg)\n",
    "    trainHistoryFile.close()\n",
    "  except:\n",
    "    pass \n",
    "\n",
    "  # Save model data every few epochs\n",
    "  if (epoch%5 == 0):\n",
    "    try:\n",
    "      modelCheckpointFilepath = os.path.join(modelCheckpointsDir, f\"checkpoint_epoch{epoch}.pth\")\n",
    "      torch.save({\n",
    "                  'epoch': epoch,\n",
    "                  'model_state_dict': modelInst.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': loss,\n",
    "                }, modelCheckpointFilepath)\n",
    "    except:\n",
    "      print(f\"Model could not be saved at epoch {epoch}\")\n",
    "\n",
    "\n",
    "# At end, print total train time, and log it\n",
    "trainEndTime = timer()\n",
    "print(f\"Total train time: {(trainEndTime-trainStartTime):.5f} sec\")\n",
    "\n",
    "try: \n",
    "  trainHistoryFile = open(trainHistoryFilepath, \"a\")\n",
    "  trainHistoryFile.write(trainEndTime)\n",
    "  trainHistoryFile.close()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "try:\n",
    "  modelFilepath = os.path.join(modelDir, \"model.pth\")\n",
    "  torch.save(modelInst.state_dict(), modelFilepath)\n",
    "except:\n",
    "  print(\"Final model could not be saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy history using pyplot\n",
    "plotFilePath = os.path.join(modelStatsDir, 'plot.png')\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.plot(trainAccuracies)\n",
    "plt.plot(testAccuracies)\n",
    "plt.plot(torch.tensor(trainLosses))\n",
    "plt.plot(torch.tensor(testLosses))\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train Acc', 'Validation Acc','Train Loss','Validation Loss'], loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(plotFilePath)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Test the model on testing data\n",
    "loss, accuracy = 0, 0\n",
    "modelInst.eval()\n",
    "with torch.inference_mode():\n",
    "  for xTest, yTest in testDataloader:\n",
    "    xTest, yTest = xTest.to(DEVICE), yTest.to(DEVICE)\n",
    "    currentBatchSize = len(xTest)\n",
    "    # 1 - Forward pass\n",
    "    testLogits = modelInst(xTest)\n",
    "    testPredictions = torch.softmax(testLogits, dim=1).argmax(dim=1) \n",
    "    # 2 - Compute batch loss, accuracy and accumulate into testLoss, testAccuracy\n",
    "    loss += loss_function(testLogits, yTest)*currentBatchSize # use logits\n",
    "    accuracy += accuracy_function(yActual=yTest, yPredicted=testPredictions)*currentBatchSize\n",
    "  \n",
    "  # Divide accumulated accuracy/loss by number of individual images, and print\n",
    "  loss /= VALIDATE_LENGTH\n",
    "  accuracy /= VALIDATE_LENGTH\n",
    "  print(f\"Test loss: {loss:.5f}, Test accuracy: {accuracy:.5f}%\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5f1159cfdad2c5db23875de314fc7465df08c3bdb8a1cc0f893debc28ccbaff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
